<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Example on Let&#39;s fuck</title>
    <link>https://langyuyi.github.io/categories/example/</link>
    <description>Recent content in Example on Let&#39;s fuck</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://langyuyi.github.io/categories/example/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>deep learning</title>
      <link>https://langyuyi.github.io/note/2024/06/14/another-note/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://langyuyi.github.io/note/2024/06/14/another-note/</guid>
      <description>神经网络基础 神经网络(Artificial Neural Network, ANN)是一种通过模拟生物神经网络进行数据拟合的机器学习算法模型，超过三层的神经网络称为深度神经网络或深度学习。它由大量的人工神经元组成，这些神经元相互连接并通过权重来传递信息。 数据归一化 数据进入神经网络前需要进行归一化步骤。常用的归一化方法有$LayerNorm$、$BatchNorm$、$RMSNorm$等。 [!attention] 注意 注意：归一化与标准化是两个不同的概念。归一化是指将数据缩放到$[0, 1]$或$[-1, 1]$区间内，标准化指的是将数据转换成均值为$0$，标准差为$1$的分布。&#xA;(1) $LayerNorm$：设$x_{ij}$表示第$i$个样本的第$j$个特征，$n$为特征数量，则$LayerNorm$的计算公式为： $$ y_{ij} = \gamma \frac{x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij}}{\sqrt{\frac{1}{n} \sum_{j=1}^{n} (x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij})^2 + \epsilon}} + \beta $$ [!note] 注释 其中，$\epsilon$是一个小常数，用来防止除以零的情况。可学习参数$\gamma$和$\beta$进行用于缩放和平移，使得模型可以学习到最佳的归一化效果。 每个元素的值减去样本每个特征的平均值，再除以标准差，最后再进行缩放和平移。&#xA;(2) $BatchNorm$：设每个特征的均值为$μ$，方差为$σ^2$，则$BatchNorm$的计算公式为 $$ y_{ij} = \gamma \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta $$ [!note] 注释 这里的$\gamma$和$\beta$是可学习的参数，对于每个特征维度都有一个对应的$\gamma$和$\beta$值。在训练过程中，这些参数会通过反向传播算法进行更新。&#xA;(3) $RMSNorm$： $$ y_{ij} = \frac{x}{\sqrt{Mean(x^2)+ε}}·γ $$ [!note] 注释 $RMSNorm$比$Layernorm$计算更简便，节约了计算速度，&#xA;前向传播过程 前向传播(Forward Propagation)指神经网络中从输入层到输出层的数据传递过程。在这个过程中，输入数据从输入神经元开始，经过多层神经元的加权求和后经过激活函数生成输出结果。具体步骤如下： ![[../封存/图片/DNN.png|400]] $$y(x)=g(\sum_{i=1}^{n}w_ix_i+b)$$ [!</description>
    </item>
    <item>
      <title>Another Note on A blogdown Tutorial</title>
      <link>https://langyuyi.github.io/note/2017/06/14/another-note/</link>
      <pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://langyuyi.github.io/note/2017/06/14/another-note/</guid>
      <description>I just discovered an awesome tutorial on blogdown written by Alison. I have to admit this is the best blogdown tutorial I have seen so far.</description>
    </item>
    <item>
      <title>A Quick Note on Two Beautiful Websites</title>
      <link>https://langyuyi.github.io/note/2017/06/13/a-quick-note/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://langyuyi.github.io/note/2017/06/13/a-quick-note/</guid>
      <description>To me, the two most impressive websites based on blogdown are:&#xA;Rob J Hyndman&amp;rsquo;s personal website. Live Free or Dichotomize by Lucy and Nick et al. I&amp;rsquo;m sure there will be more.</description>
    </item>
    <item>
      <title>A Plain Markdown Post</title>
      <link>https://langyuyi.github.io/post/2016/02/14/a-plain-markdown-post/</link>
      <pubDate>Sun, 14 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://langyuyi.github.io/post/2016/02/14/a-plain-markdown-post/</guid>
      <description>This sample post is mainly for blogdown users. If you do not use blogdown, you can skip the first section.&#xA;1. Markdown or R Markdown This is a post written in plain Markdown (*.md) instead of R Markdown (*.Rmd). The major differences are:&#xA;You cannot run any R code in a plain Markdown document, whereas in an R Markdown document, you can embed R code chunks (```{r}); A plain Markdown post is rendered through Goldmark by default, and an R Markdown document is compiled by rmarkdown and Pandoc.</description>
    </item>
    <item>
      <title>Lorem Ipsum</title>
      <link>https://langyuyi.github.io/post/2015/07/23/lorem-ipsum/</link>
      <pubDate>Thu, 23 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://langyuyi.github.io/post/2015/07/23/lorem-ipsum/</guid>
      <description>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&#xA;Quisque mattis volutpat lorem vitae feugiat.</description>
    </item>
  </channel>
</rss>
