<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Chinese | My Deep Learning Note | My Page</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">Log</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/404.html">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Chinese | My Deep Learning Note</span></h1>
<h2 class="author">Yuyi Lao</h2>
<h2 class="date">2024/09/24</h2>
</div>

<main>
<h4 id="神经网络基础">神经网络基础</h4>
<ul>
<li>神经网络(Artificial Neural Network, ANN)是一种通过模拟生物神经网络进行数据拟合的机器学习算法模型，超过三层的神经网络称为深度神经网络或深度学习。它由大量的人工神经元组成，这些神经元相互连接并通过权重来传递信息。</li>
</ul>
<h5 id="数据归一化">数据归一化</h5>
<ul>
<li>数据进入神经网络前需要进行归一化步骤。常用的归一化方法有$LayerNorm$、$BatchNorm$、$RMSNorm$等。</li>
</ul>
<blockquote>
<p>[!attention] 注意
注意：归一化与标准化是两个不同的概念。归一化是指将数据缩放到$[0, 1]$或$[-1, 1]$区间内，标准化指的是将数据转换成均值为$0$，标准差为$1$的分布。</p>
</blockquote>
<p>(1) $LayerNorm$：设$x_{ij}$表示第$i$个样本的第$j$个特征，$n$为特征数量，则$LayerNorm$的计算公式为：
</p>
$$
y_{ij} = \gamma \frac{x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij}}{\sqrt{\frac{1}{n} \sum_{j=1}^{n} (x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij})^2 + \epsilon}} + \beta
$$
<blockquote>
<p>[!note] 注释
其中，$\epsilon$是一个小常数，用来防止除以零的情况。可学习参数$\gamma$和$\beta$进行用于缩放和平移，使得模型可以学习到最佳的归一化效果。
每个元素的值减去样本每个特征的平均值，再除以标准差，最后再进行缩放和平移。</p>
</blockquote>
<p>(2) $BatchNorm$：设每个特征的均值为$μ$，方差为$σ^2$，则$BatchNorm$的计算公式为
</p>
$$
y_{ij} = \gamma \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$
<blockquote>
<p>[!note] 注释
这里的$\gamma$和$\beta$是可学习的参数，对于每个特征维度都有一个对应的$\gamma$和$\beta$值。在训练过程中，这些参数会通过反向传播算法进行更新。</p>
</blockquote>
<p>(3) $RMSNorm$：
</p>
$$
y_{ij} = \frac{x}{\sqrt{Mean(x^2)+ε}}·γ
$$
<blockquote>
<p>[!note] 注释
$RMSNorm$比$Layernorm$计算更简便，节约了计算速度，</p>
</blockquote>
<h5 id="前向传播过程">前向传播过程</h5>
<ul>
<li>前向传播(Forward Propagation)指神经网络中从输入层到输出层的数据传递过程。在这个过程中，输入数据从输入神经元开始，经过多层神经元的加权求和后经过激活函数生成输出结果。具体步骤如下：
![[../封存/图片/DNN.png|400]]
$$y(x)=g(\sum_{i=1}^{n}w_ix_i+b)$$</li>
</ul>
<blockquote>
<p>[!NOTE]
其中，$y(x)$表示神经网络的输出，$g$表示‌激活函数，$w_i$表示第$i$个输入数据的权重，$x_i$表示第$i$个输数据号，$b$为偏置项。前向传播的主要目的是基于当前的权重和偏置参数，计算模型对于给定输入的预测输出。</p>
</blockquote>
<ul>
<li>
<p><strong>激活函数</strong></p>
<p>在神经网络中除了线性层外往往还需要加入激活函数，没有激活函数的神经网络只是线性函数，无法拟合曲线，任何一条曲线都能用多条线性函数来逼近，而激活函数使神经网络具有分段的能力，根据万能逼近原理(universal approximation theorrm)，3层神经网络可以逼近任意曲线<a href="https://zhuanlan.zhihu.com/p/385531651"></a>。以下是神经网络中最常见的三个激活函数：
(1) $Sigmoid$、$tanh$、$Relu$激活函数：
</p>
$$
\begin{align}
	&Sigmoid(x)=\frac{1}{1+e^{-x}}\\
	&Tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
	&Relu(x)=
	\begin{cases}
		x,x≥0\\
		0,x<0
	\end{cases}
\end{align}
$$
</li>
</ul>
<blockquote>
<p>[!NOTE] 注释
$Relu$函数反向传播求导简单，减少计算量</p>
</blockquote>
<p>(2) $GLU$激活函数：
</p>
$$GLU(x)=Sigmoid(W_1x+b_1)⊗(W_2x+b_2) $$
<blockquote>
<p>[!note] 注释
$x$是输入向量，$⊗$表示逐元素相乘。$GLU$函数将输入向量通过两个不同的线性变换，得到了两个不同的向量，其中一个向量不经过激活函数，另一个向量经过$Sigmoid$函数后其元素被映射到$0-1$之间，若元素为$1$，则另一向量对应位置的元素不变，若为零则不允许通过，感觉有点类似于$LSTM$的门控结构，个人理解$GLU$的作用类似于$LSTM$中的遗忘门。</p>
</blockquote>
<p>(3) $Swish$激活函数：
</p>
$$Swish(x)=x⊗Sigmoid(βx)$$
<blockquote>
<p>[!note] 注释
$𝛽$是一个可学习的超参数，默认为$1$。$Swish$函数的独特之处在于其非线性和自适应性，当输入接近$0$时，行为类似$ReLU$函数，而在其他区域，其斜率可以根据输入值进行自我调整。相比传统的$ReLU$无法调整的一条直线更好。</p>
</blockquote>
<p>(4) $SwiGLU$激活函数：
</p>
$$SwiGLU(x)=Swish(W_1x+b_1)⊗(W_2x+b_2) $$
<blockquote>
<p>[!note] 注释
$SwiGLU$函数相当于把$GLU$和$Swish$结合在了一起，把$GLU$当中的$Sigmoid$换成了$Swish$，结合了二者的优点，$1+1&gt;2$。
$SiLU$函数：LlaMa2模型采用的是$SiLU$函数，$SiLU$就是$β=1$时的$Swish$与$GLU$结合的$SwiGLU$函数</p>
</blockquote>
<ul>
<li>
<p><strong>损失函数</strong></p>
<p>神经网络通过计算预测值与真实值之间的误差，再将误差进行反向传播，从而更新权重。</p>
<p>(1) 常见损失函数：常用的损失函数有$MSE$、$RMSE$、$MAE$等，计算公式如下：
</p>
$$
\begin{align}
	&\text{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2\\
	&\text{RMSE}=\sqrt{MSE}\\
	&\text{MAE}=\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y}_i|
\end{align}	
$$
</li>
</ul>
<blockquote>
<p>[!NOTE] 注释
其中，$y_i$是第$i$个真实值，$\hat{y}_i$是模型第$i$个预测值，$n$为数据总数。</p>
<ul>
<li>$MSE$是预测值与真实值之差的平方的平均值，函数通过差值的平方放大误差的影响；</li>
<li>$RMSE$实际上是$MSE$的平方根，$RMSE$保留了$MSE$的特性，但它将误差单位转换回原始数据的尺度，使得解释更加直观；</li>
<li>$MAE$是预测值与真实值之差的绝对值的平均值。它比$MSE$更能抵抗异常值的影响，因为绝对值不会像平方那样放大误差。</li>
</ul>
</blockquote>
<p>(2) 交叉熵损失 (Cross-Entropy Loss)：</p>
<p>不同的问题交叉熵损失的计算公式也不同，对于二分类问题，交叉熵损失的计算公式如下：
</p>
$$\text{CE}=-\frac{1}{n}\sum_{i=1}^{n}[y_i\log(p_i) + (1-y_i)\log(1-p_i)]$$
<blockquote>
<p>[!note] 注释
其中，$p_i$是模型预测第$i$个观测属于正类的概率，$y_i$是第$i$个观测的真实标签（通常为$0$或$1$）。</p>
</blockquote>
<p>对于多分类问题，交叉熵损失的计算公式如下：
</p>
$$\text{CE} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{ic}\log(p_{ic})$$
<blockquote>
<p>[!NOTE] 注释
其中，$C$是类别的数量，$y_{ic}$是指示变量，若第$i$个观测属于第$c$类，则为$1$，否则为$0$，$p_{ic}$是模型预测第$i$个观测属于第$c$类的概率。
交叉熵损失用于分类问题，特别是当输出是概率分布时。它度量了两个概率分布之间的差异，鼓励模型预测与真实标签尽可能接近的概率分布。</p>
</blockquote>
<h5 id="反向传播过程">反向传播过程</h5>
<ul>
<li>神经网络的权重更新通过反向传播实现<a href="https://zhuanlan.zhihu.com/p/289184911"></a>，参见周志华-机器学习p101</li>
</ul>
<hr>
<h4 id="卷积神经网络">卷积神经网络</h4>
<blockquote>
<p>[!Info]-  略
卷积神经网络：卷积神经网络一般可分为卷积层、池化层和输出层三个阶段
重要的卷积神经网络变体：
U-net：
![[../封存/图片/unet 1.png|400]]
左边是Encoder，右边是Decoder，之间通过残差连接确保不会丢失信息，具体细节见原始模型代码：<a href="https://github.com/milesial/Pytorch-UNet">github</a></p>
</blockquote>
<hr>
<h4 id="循环神经网络">循环神经网络</h4>
<p>循环神经网络<a href="https://www.bilibili.com/video/BV1z5411f7Bm/?spm_id_from=333.337.search-card.all.click"></a>（Recurrent Neural Network, RNN）又叫递归神经网络，很简单，相比于普通的全连接神经网络就是把上一次训练的权重与这一次训练的输出值同时输入神经网络。循环神经网络的结构如下：</p>
<p>![[../封存/图片/Pasted image 20240603002024.png|400]]</p>
<ul>
<li><strong>计算公式：</strong>
$$Y_n=softmax(\sum^{n}_{n=1}tanh(UX_n​+WS_{n−1}​))$$</li>
</ul>
<blockquote>
<p>[!note] 注释
$X_n$为输入内容，$Y_n$为输出内容，$U、V、W$为权重，$S_n​$为神经元，$c$为输出的个数​.
根据问题的需要，循环神经网络的隐层可以是全连接的，也可以是一对一的。此外，假设每一层级之间的权重是相同的，可降低计算量。循环神经网络的常见变体有长短时记忆神经网络和门控循环单元。</p>
</blockquote>
<h5 id="长短时记忆神经网络">长短时记忆神经网络</h5>
<p>长短时记忆神经网络(long short-term memory neural network, LSTM)是一种主要用于处理时序问题的神经网络结构，由循环神经网络改进而来。相比于传统的循环神经网络，长短时记忆神经网络能够深入挖掘时间序列数据中的固有规律并具有长时记忆功能。$LSTM$的结构包括输入门$i_t$，遗忘门$f_t$，输出门$o_t$，其结构图如下：</p>
<p>![[../封存/图片/lstm.png|400]]</p>
<ul>
<li><strong>遗忘门：</strong>
$$f_t=σ(W_{xf}x_t+b_{xf}+W_{hf}h_{t-1}+b_{hf})$$</li>
</ul>
<blockquote>
<p>[!note] 注释
式中，输入值$x_t$和上一时刻的输出值$h_{t-1}$经过$Sigmoid$激活函数后，生成一个$0$~$1$之间的向量，遗忘门通过对记忆细胞的点乘操作对记忆内容进行选择性遗忘，若向量中某元素的值为$0$，则经过点乘后记忆细胞中对应值全部遗忘，若向量中某元素的值为$1$，则全部保留。$W_{xf}$和$W_{hf}$分别为遗忘门的权重，$b_{xf}$和$b_{hf}$为偏置项。</p>
</blockquote>
<ul>
<li><strong>记忆门：</strong>
$$i_t=σ(W_{xi}x_t+b_{xi}+W_{hi}h_{t-1}+b_{hi})$$
$$g_t=tanh(W_{xg}x_t+b_{xg}+W_{hg}h_{t-1}+b_{hg})$$</li>
</ul>
<blockquote>
<p>[!note] 注释
式中，输入值$x_t$和上一时刻的输出值$h_{t-1}$经过$Sigmoid$激活函数后，生成一个0~1之间的向量，输入值$x_t$和上一时刻的输出值$h_{t-1}$再经过$tanh$激活函数后放缩到$[-1, 1]$成为候选细胞，候选细胞只是起到一个过渡的作用，两者之间经过元素相乘运算清除部分信息，这一步相当于选择要记忆什么内容，再经过与记忆细胞的元素相加运算将候选细胞的记忆内容记忆加到记忆细胞中。$W_{xi}$和$W_{hi}$分别为记忆门的权重，$b_{xi}$和$b_{hi}$为偏置项。$W_{xg}$和$W_{hg}$为候选细胞权重，$b_{xg}$和$b_{hg}$为偏置项。</p>
</blockquote>
<ul>
<li><strong>记忆细胞：</strong>
$$c_t=f_t⊙c_{t-1}+i_t⊙g_t$$</li>
</ul>
<blockquote>
<p>[!note] 注释
记忆细胞代表长期记忆，经过与遗忘门的点乘操作选择性遗忘和与记忆门的元素相加操作选择性记忆得到。</p>
</blockquote>
<ul>
<li><strong>输出门：</strong>
$$o_t=σ(W_{xo}x_t+b_{xo}+W_{ho}h_{t-1}+b_{ho})$$
$$h_t=o_t⊙tanh(c_t)$$</li>
</ul>
<blockquote>
<p>[!note] 注释
输出门选择要输出什么内容，输入值$x_t$和上一时刻的输出值$h_{t-1}$经过sigmoid激活函数后生成元素为$0$~$1$之间的向量。$W_{xo}$和$W_{ho}$为权重，$b_{xo}$和$b_{ho}$为偏置项。输入门再与记忆细胞进行点乘操作，将记忆细胞中的信息提取出来，得到最终输出$h_t$，$h_t$再参与到下一时刻的训练。</p>
</blockquote>
<ul>
<li><strong>具体流程：</strong>
![[../封存/图片/lstmgif图.gif|400]]</li>
</ul>
<blockquote>
<p>[!note]
LSTM的门展开后就是Dense层
LSTM应该记忆什么遗忘什么取决于门的权重，由网络训练得到</p>
</blockquote>
<h5 id="门控循环单元">门控循环单元</h5>
<p>门控循环单元(gated recurrent unit, GRU)由长短时记忆神经网络改进而来，门控循环单元与长短时记忆神经网络效果相似，但门控循环单元模型更简洁，$GRU$的结构与计算公式如下所示：</p>
<p>![[../封存/图片/gru.png|400]]</p>
<ul>
<li><strong>重置与更新门：</strong>
$$ R_t​=σ(X_t​W_{xr}​+H_{t−1}​W_{hr}​+b_r​)$$
$$ Z_t=σ(X_tW_{xz}+H_{t-1}W_{hz}+b_z)$$</li>
</ul>
<blockquote>
<p>[!note] 注释
$R_t, Z_t$分别为时刻$t$的重置门和更新门，$H_t$是时刻$t$的输出，$X_t$是时刻$t$的输入，$W_{xr}和W_{hr}$为重置门的权重，$b_r和b_r$为重置门的偏置项，$W_{xz}和W_{hz}$为更新门的权重，$b_z和b_z$为更新门的偏置项，$R_t, Z_t$均为0~1之间的向量。</p>
</blockquote>
<ul>
<li><strong>隐藏状态</strong>
$$ \tilde H_t=tanh(X_t​W_{xh​}+(R_t​⊙H_{t−1}​)W_{hh​}+b_h$$
$$ H_t=Z_t​⊙H_{t−1}​+(1−Z_t​)⊙\tilde H_t$$</li>
</ul>
<blockquote>
<p>[!note] 注释
$\tilde H_t$为候选隐藏状态，$Z_t$表示遗忘，反之$(1−Z_t​)$表示记忆，$W_{xh​}$和$W_{hh​}$为隐藏状态的权重，$b_h$为隐藏状态的偏置项。
有权重的地方就表明这里有一个$Dense$层。如果将重置门$R_t$设为$1$，更新门设为$Z_t$设为$0$，就是一个标准的循环神经网络。</p>
</blockquote>
<ul>
<li><strong>具体流程：</strong>
![[../封存/图片/grugif图.gif|400]]</li>
</ul>
<blockquote>
<p>[!CITE]- 参考资料
<a href="https://blog.csdn.net/zyf918/article/details/136172798">https://blog.csdn.net/zyf918/article/details/136172798</a></p>
</blockquote>
<hr>
<h4 id="transformer">Transformer</h4>
<ul>
<li>
<p>Transformer结构框架图：![[../封存/图片/transformer.png|250]]</p>
</li>
<li>
<p><strong>输入部分</strong></p>
<p>(1) 词嵌入($embedding$)：</p>
<blockquote>
<p>[!note] 注释
两边的$Inputs$和$Outputs$分别代表样本和标签数据，$Embedding$是一个单层的全连接层，$Inputs$和$Outputs$经过$Embedding$的转换后可以表示单词或者语句的语义</p>
</blockquote>
<p>(2) 位置编码($positional$ $encoding$)：</p>
<p>词向量偶数位置使用$\sin$，奇数位置使用$\cos$，将位置信息嵌入词向量中，原始版本的Transformer采用三角函数编码($Sinusoidal$ $Encoding$)：
</p>
$$PE_{(pos,2i)}=\sin(\frac{pos}{10000^{2i/d_{model}}})$$
$$PE_{(pos,2i+1)}=\cos(\frac{pos}{10000^{2i/d_{model}}})$$
$$
\begin{matrix}
	sin & cos & sin & cos & sin & cos\\
	↓ & ↓ & ↓ & ↓ & ↓ & ↓ &\\
	[0, & 1, & 2, & 3, & 4, & 5, & \cdots &]
\end{matrix}
$$
</li>
</ul>
<blockquote>
<p>[!note] 注释
$pos$表示位置索引（从$0$开始），$𝑖$是词嵌入向量的维度索引，$𝑑_{𝑚𝑜𝑑𝑒𝑙}$是模型中词嵌入向量的维度。$带有位置信息的词embedding = 原始的词embedding+词embedding对应的PE计算结果$。RNN和LSTM是将文本一个词一个词输入网络，而transformer是并行计算的，意味着词与词之间不存在顺序关系，所以需要将位置信息嵌入词向量中，PE嵌入的是却对位置信息，通过频率、波长与位置的关系把信息嵌入词向量中。</p>
</blockquote>
<ul>
<li>
<p><strong>Encoder部分</strong></p>
<p>整个Transformer可以分为$Encoder$和$Decoder$两个部分。其中，$Encoder$可分成输入部分、注意力机制和前馈神经网络三个部分</p>
<p>(1) 自注意力机制($Self$-$Attention$)：<a href="https://zhuanlan.zhihu.com/p/410776234">zhihu</a></p>
<p>$Q(query)、K(key)、V(value)以及Attention$的计算：
</p>
$$Q = X \times W^Q$$
$$K = X \times W^K$$
$$V = X \times W^V$$
$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
</li>
</ul>
<blockquote>
<p>[!note] 注释
$X$是经过词嵌入与位置编码后的输入，$W^Q$、$W^K$、$W^V$是三个可被训练的参数矩阵。$d_k$是键对应的向量维度，即$embedding$的维度。关于$Q、K、V$的作用：<a href="https://www.zhihu.com/question/325839123">※</a>。$QK^T$表示查询权重，$Q$乘$K^T$相当于$Q$与$K$做数量积操作，数量级可以反映两个向量的相似度，结果越大表明两个向量的信息相似度越高，$softmax$的作用是确保权重之和为$1$，为了避免规模差异对相似度计算产生影响，通常会对向量进行缩放。为什么要除以$\sqrt{d_k}$？</p>
</blockquote>
<p>(2) 多头自注意力机制($Multi$-$Head$ $Self$-$Attention$)：<a href="https://blog.csdn.net/weixin_45662399/article/details/134384186">CSDN</a></p>
<p>自注意力机制在对当前位置信息进行编码时，会过度的将注意力集中于自身的位置，因此引入多头自注意力机制。多头自注意力机制的步骤如下：</p>
<p>首先，定义$n$组$W^Q_n、W^K_n和W^V_n$，生成多组$Q_n、K_n和V_n:$
</p>
$$Q_n = X \times W^Q_n$$
$$K_n = X \times W^K_n$$
$$V_n = X \times W^V_n$$
<p>
对多组$Q、K、V$进行$attention$计算，生成多个$Attention:$
</p>
$$Attention_n(Q_n,K_n,V_n)=softmax(\frac{Q_nK_n^T}{\sqrt{d_k}})V_n$$
<p>
将多组$Attention(Attention_0,Attention_1,&hellip;,Attention_n)$进行拼接(cancat)，再乘以矩阵$W^O$做一次线性变换降低维度，得到最终的$MultiHead:$
</p>
$$MultiHead = Concat(Attention_0,Attention_1,...,Attention_n)W^O$$
<blockquote>
<p>[!note] 注释
<code>nhead</code>需要能被<code>d_model</code>整除，因为所有头的维度之和等于<code>d_model</code>。即每个注意力头的输出维度是<code>d_model/nhead</code>.</p>
</blockquote>
<p>(3) 残差结构与前馈神经网络：</p>
<p>前馈神经网络公式：
</p>
$$FFN(x) = max(0,xW_1+b_1)W_2+b_2$$
<blockquote>
<p>[!note] 注释
激活函数为ReLU。残差结构参考<a href="https://blog.csdn.net/qimo601/article/details/127410607">CSDN</a>。</p>
</blockquote>
<ul>
<li>
<p><strong>Dncoder部分</strong></p>
<p>(1) $Masked$ $Multi$-$Head$ $Self$-$Attention$：</p>
<p>$mask$-$attention$的计算公式如下：
</p>
$$Attention_{mask,n}(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}+mask)V$$
$$
mask = 
\begin{pmatrix}
	0 & -∞ & -∞ & \cdots & -∞ \\
	0 & 0 & -∞ & \cdots & -∞ \\
	0 & 0 & 0 & \cdots & -∞\\
	\vdots &\vdots& \vdots&\ddots &\vdots\\
	0 & 0 & 0 & \cdots & 0 
\end{pmatrix}
$$
</li>
</ul>
<blockquote>
<p>[!note] 注释
由于transformer在训练时是将数据直接全部输入网络，所以需要使decoder看不到未来的信息。计算很简单，就是加一个mask矩阵</p>
</blockquote>
<p>(2) $Cross$-$Attention$：</p>
<p>$Cross$-$attention$的计算公式如下：
</p>
$$Attention_n(Q_{Decoder},K_{Encoder},V_{Encoder})=softmax(\frac{QK^T}{\sqrt{d_k}}+mask)V$$
<blockquote>
<p>[!note] 注释
Encoder的最后一个模块的输出，输入进每个Decoder中，作为Decoder Multi-Head Self-Attention的$Key$和$Value$</p>
</blockquote>
<h5 id="transformer相关变体">Transformer相关变体</h5>
<ul>
<li>
<p>$LLaMA$</p>
<p>$LLaMA$是由$Meta$公司开源的大语言模型，于$2023$年$2$月$25$日发布，包括$7B、13B和70B$三个版本。$LLaMA$是Transformer的变体，相比Transformer，其结构与特点如下：</p>
<p>(1) $Encoder$-$Decoder$ $\Rightarrow$ $Decoder$-$Only$：</p>
</li>
</ul>
<blockquote>
<p>[!NOTE] 注释
Llama模型仅采用了Transfomer的解码器结构，相比Transfomer的样本和标签数据分别从编码器和解码器输入，$LLaMA$直接将样本和标签拼接在一起输入到神经网络</p>
</blockquote>
<p>(2) $LayerNorm$ $and$ $Post$-$norm$ $\Rightarrow$ $RMSNorm$ $and$ $Pre$-$norm$：</p>
<blockquote>
<p>[!NOTE] 注释
相比Transform的$LayerNorm$，$LLaMA$模型采用$RMSNorm$，$RMSNorm$比$Layernorm$计算更简便，节约了计算速度，此外，$LLaMA$还将$nomilization$层放在了注意力的前面，为啥不知道</p>
</blockquote>
<p>(3) 三角函数编码 → 旋转位置编码(RoPE)：<a href="http://arxiv.org/abs/2104.09864">论文</a><a href="https://kexue.fm/archives/10040">科学空间</a></p>
<p>以二维词向量为例，将自注意力计算过程中的的$q$和$k$与二维旋转矩阵$R^{2×2}$相乘：
</p>
$$
R_n^{2×2} = 
\begin{pmatrix}
	cos(nθ) & -sin(nθ) \\
	sin(nθ) & cos(nθ)
\end{pmatrix}
$$
$$(R_m^{2×2}q)^⊤(R_n^{2×2}k)=q^{⊤}(R^{2×2}_m)^TR_n^{2×2}k=q^⊤R_{n−m}^{2×2}k$$
<blockquote>
<p>[!note] 注释
其中，$θ$是一个常量，它的具体原理是让词向量在空间上发生旋转，从而把位置信息嵌入到词向量中，这也是为什么它叫旋转位置编码。可以看到相对位置信息$(n−m)$在$q$与$k$相乘后嵌入到了词向量中，形象过程可以看下图：</p>
</blockquote>
<p>![[../封存/图片/RoPE.png|500]]</p>
<ul>
<li>词向量一般不可能是二维，要想对高维词向量进行旋转，需要将旋转矩阵拓展到高维：</li>
</ul>
$$R_n^{d×d} =
\begin{pmatrix}
	cos(nθ_0) & -sin(nθ_0) & 0 & 0 & \cdots & 0 & 0\\
	sin(nθ_0) & cos(nθ_0) & 0 & 0 & \cdots & 0 & 0\\
	0 & 0 & sin(nθ_1) & cos(nθ_1) & \cdots & 0 & 0\\
	0 & 0 & sin(nθ_1) & cos(nθ_1) & \cdots & 0 & 0\\
	\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & 0 & \cdots & cos(nθ_{d/2-1}) & -sin(nθ_{d/2-1})\\
	0 & 0 & 0 & 0 & \cdots & sin(nθ_{d/2-1}) & cos(nθ_{d/2-1})\\
\end{pmatrix}
$$
<ul>
<li>可以应用分块矩阵将旋转矩阵简化：</li>
</ul>
$$
\begin{align}
R_n^{d×d}
&=
\left( 
\begin{array}{cc|ccc}
	cos(nθ_0) & -sin(nθ_0) & 0 & 0 & \cdots & 0 & 0\\
	sin(nθ_0) & cos(nθ_0) & 0 & 0 & \cdots & 0 & 0\\
	\hline 
	0 & 0 & sin(nθ_1) & cos(nθ_1) & \cdots & 0 & 0\\
	0 & 0 & sin(nθ_1) & cos(nθ_1) & \cdots & 0 & 0\\
	\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & 0 & \cdots & cos(nθ_{d/2-1}) & -sin(nθ_{d/2-1})\\
	0 & 0 & 0 & 0 & \cdots & sin(nθ_{d/2-1}) & cos(nθ_{d/2-1})
\end{array}
\right)\\\\
&= 
\begin{pmatrix}
	R_{(0)} & 0 & 0 & \cdots & 0\\
	0 & R_{(1)} & 0 & \cdots & 0\\
	0 & 0 & R_{(2)} & \cdots & 0\\
	0 & 0 & 0 & \ddots & \vdots\\
	0 & 0 & 0 & \cdots & R_{(d/2-1)}
\end{pmatrix}
\end{align}
$$
<ul>
<li>计算公式改写为以下形式，不展开了：
$$(R_m^{d×d}q)^⊤(R_n^{d×d}k)=q^{⊤}(R^{d×d}_m)^TR_n^{d×d}k=q^⊤R_{n−m}^{d×d}k$$</li>
</ul>
<blockquote>
<p>[!note] 注释
prompt的理解是另外一个小型的训练样本适时的调整大模型的参数</p>
</blockquote>
<ul>
<li>
<p>$BERT$</p>
<p>(1) $Encoder$-$Decoder$ $\Rightarrow$ $Encoder$-$Only$：</p>
</li>
</ul>
<blockquote>
<p>[!NOTE] 注释
BERT模型就是Transformer左边的Encodeer结构，2018年10⽉由Google AI研究院提出，其架构图如下：</p>
</blockquote>
<p>![[../封存/图片/Pasted image 20240613235246.png|600]]</p>
<ul>
<li>
<p>$GPT$</p>
<p>(1) $Encoder$-$Decoder$ $\Rightarrow$ $Decoder$-$Only$：</p>
</li>
</ul>
<blockquote>
<p>[!note] 注释
GPT模型，全称为‌(Generative Pre-trained Transformer)，是由‌OpenAI团队开发的一种基于深度学习的自然语言处理模型。GPT使用Transformer的Decoder作为其架构，但在Decoder的基础上做了一些改动，去掉了Cross-Attention。</p>
</blockquote>
<p>![[封存/图片/Pasted image 20240831160909.png|200]]</p>
<ul>
<li>
<p>$DDPM$</p>
<p>从噪声生成图片，分为加噪和去噪两个步骤</p>
<p>(1) Diffusion Process (Forward Process)</p>
<p>数学原理：</p>
<p>$x_t与x_{t-1}$的关系如下：
</p>
$$
x_t = \sqrt{β_t}×ε_t+\sqrt{1-β_t}×x_{t-1}
$$
</li>
</ul>
<blockquote>
<p>[!note] 注释
$ε_t$是符合高斯分布的高斯噪声</p>
</blockquote>
<p>为了简化公式，设$α_t = 1-β_t$，则$x_t与x_{t-1}$的关系如下：
</p>
$$x_t = \sqrt{1-α_t}×ε_t+\sqrt{α_t}×x_{t-1}$$
<p>
$x_t与x_{t-2}的关系如下$：
</p>
$$x_t = \sqrt{1-α_t}×ε_t+\sqrt{α_t}×(\sqrt{1-α_{t-1}}×ε_{t-1}+\sqrt{α_{t-1}}×x_{t-2})$$
<p>
以此类推，得到公式：</p>
<p>部件架构：</p>
<p>(2) Denoise Process (Reverse Process)</p>
<ul>
<li>
<p>$LDM$</p>
<p>又称Stable Diffusion</p>
</li>
</ul>
<p>![[../封存/图片/diffusion.png|500]]</p>
<blockquote>
<p>[!CITE]- 参考资料
<a href="https://www.bilibili.com/video/BV1Di4y1c7Zm/?share_source=copy_web&amp;vd_source=59e03da0555efae2883b4a6f21e8f47b">Transformer从零详细解读(可能是你见过最通俗易懂的讲解)</a>
<a href="https://www.bilibili.com/video/BV1UL411g7aX/?spm_id_from=333.337.search-card.all.click&amp;vd_source=3749948871311220e961685a5ac2a7a7">手推transformer_哔哩哔哩</a>
![[../封存/图片/mlkv-Transformer.png]]</p>
</blockquote>
<hr>
<h4 id="深度学习前沿技术">深度学习前沿技术</h4>
<p>深度学习前沿的但未经过时间检验的技术。</p>
<h5 id="mambaselective-state-space-model-mamba">Mamba(Selective State Space Model, Mamba)</h5>
<p><strong>状态空间模型(State Space Model, SSM)</strong></p>
<p>状态空间模型是一种用于分析和预测时间序列数据的统计模型。它将系统的动态行为表示为一组状态变量的演化，以及这些状态变量与观测到的数据之间的关系。状态空间模型通常由两部分组成：状态方程和观测方程。</p>
<p>![[../封存/图片/ssm.png]]</p>
<ul>
<li>
<p><strong>状态方程</strong></p>
<p>状态方程描述了系统内部状态随时间变化的规律，可以是线性的也可以是非线性的。对于线性高斯状态空间模型，状态方程可以写作：
</p>
$$h_t = A h_{t-1} + B x_t + w_t$$
</li>
</ul>
<blockquote>
<p>[!NOTE] 注释
其中，$h_t$是在时刻$t$的状态向量。$A$是状态转移矩阵，描述状态向量如何从$t-1$时刻转移到$t$时刻。，$B$是控制输入矩阵，$x_t$是在时刻$t$的控制输入向量。$w_t$是状态扰动项</p>
</blockquote>
<ul>
<li>
<p><strong>观测方程</strong></p>
<p>观测方程描述了如何从系统的状态得到观测数据，同样可以是线性的或非线性的。对于线性高斯状态空间模型，观测方程可以写作：
</p>
$$y_t = C h_t + Dx_t+v_t$$
</li>
</ul>
<blockquote>
<p>[!NOTE] 注释
其中，$y_t$ 是在时刻 $t$ 的观测向量，$C$ 是观测矩阵，描述状态向量如何映射到观测数据，$v_t$ 是观测噪声。ABCD都是常数矩阵，时间变化无关</p>
</blockquote>
<ul>
<li>
<p><strong>离散化</strong></p>
</li>
<li>
<p><strong>卷积化</strong></p>
</li>
</ul>
<p><strong>选择状态空间模型(Selective State Space Model, Mamba)</strong></p>
<blockquote>
<p>[!CITE]- 参考资料
<a href="https://zhuanlan.zhihu.com/p/685994970">https://zhuanlan.zhihu.com/p/685994970</a>
<a href="https://www.zhihu.com/question/644981978/answer/3405813530">https://www.zhihu.com/question/644981978/answer/3405813530</a></p>
</blockquote>
<h5 id="kankolmogorov-arnold-networks">KAN(Kolmogorov-Arnold Networks)</h5>
<p><strong>柯尔莫哥洛夫-阿诺德网络</strong></p>
<ul>
<li>
<p><strong>理论基础：</strong></p>
<p>$Kolmogorov$-$Arnold$表示定理：任何一个多变量函数都可以表示为一堆单变量函数的集合。
</p>
$$f(x_1, x_2, ..., x_n) = \sum_{i=1}^{2n+1} \Phi_i(\sum_{j=1}^{n} \phi_{ij}(x_j))$$
</li>
</ul>
<blockquote>
<p>[!NOTE] 注释
其中$\Phi_i$和$\phi_{ij}$都是单变量函数。虽然理论上这个定理保证了函数的表示可能性，但在实际操作中找到合适的$\Phi_i$和$\phi_{ij}$函数仍然是一个挑战。此外，定理并没有给出构造这些函数的具体方法，因此在实践中需要依赖其他技术和算法来实现。</p>
</blockquote>
<p>MLP的缺点：</p>
<ol>
<li>梯度消失与梯度爆炸</li>
<li>参数效率低</li>
<li>处理高维数据能力有限</li>
<li>长期依赖问题</li>
</ol>

</main>

  <footer>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script src="//cdn.jsdelivr.net/combine/npm/katex/dist/katex.min.js,npm/katex/dist/contrib/auto-render.min.js,npm/@xiee/utils/js/render-katex.js" defer></script>

<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>

  
  <hr/>
  © <a href="https://github.com/laoyuyi">Yuyi Lao</a> 2024 | <a href="https://github.com/laoyuyi">Github</a> | <a href="https://agis.caas.cn/kydw/kydwyjzx/hcswxyjzx/e4ae1196caaa472399425abe23e90695.htm">zc-lab</a>
  
  </footer>
  </body>
</html>

