<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial on My Page</title>
    <link>https://langyuyi.github.io/tags/tutorial/</link>
    <description>Recent content in Tutorial on My Page</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://langyuyi.github.io/tags/tutorial/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chinese | My Deep Learning Note</title>
      <link>https://langyuyi.github.io/note/2024/09/24/another-note/</link>
      <pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://langyuyi.github.io/note/2024/09/24/another-note/</guid>
      <description>神经网络基础 神经网络(Artificial Neural Network, ANN)是一种通过模拟生物神经网络进行数据拟合的机器学习算法模型，超过三层的神经网络称为深度神经网络或深度学习。它由大量的人工神经元组成，这些神经元相互连接并通过权重来传递信息。 数据归一化 数据进入神经网络前需要进行归一化步骤。常用的归一化方法有\(LayerNorm\)、\(BatchNorm\)、\(RMSNorm\)等。 注意：归一化与标准化是两个不同的概念。归一化是指将数据缩放到\([0, 1]\)或\([-1, 1]\)区间内，标准化指的是将数据转换成均值为$0$，标准差为$1$的分布。&#xA;(1) \(LayerNorm\)：设\(x_{ij}\)表示第\(i\)个样本的第\(j\)个特征，\(n\)为特征数量，则LayerNorm的计算公式为： $$ y_{ij} = \gamma \frac{x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij}}{\sqrt{\frac{1}{n} \sum_{j=1}^{n} (x_{ij} - \frac{1}{n} \sum_{j=1}^{n} x_{ij})^2 + \epsilon}} + \beta $$ 其中，$\epsilon$是一个小常数，用来防止除以零的情况。可学习参数\(\gamma\)和\(\beta\)进行用于缩放和平移，使得模型可以学习到最佳的归一化效果。 每个元素的值减去样本每个特征的平均值，再除以标准差，最后再进行缩放和平移。&#xA;(2) \(BatchNorm\)：设每个特征的均值为μ，方差为σ^2，则BatchNorm的计算公式为 $$ y_{ij} = \gamma \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta $$ 这里的\gamma和\beta是可学习的参数，对于每个特征维度都有一个对应的\gamma和\beta值。在训练过程中，这些参数会通过反向传播算法进行更新。&#xA;(3) RMSNorm： $$ y_{ij} = \frac{x}{\sqrt{Mean(x^2)+ε}}·γ $$ RMSNorm比Layernorm计算更简便，节约了计算速度，&#xA;前向传播过程 前向传播(Forward Propagation)指神经网络中从输入层到输出层的数据传递过程。在这个过程中，输入数据从输入神经元开始，经过多层神经元的加权求和后经过激活函数生成输出结果。具体步骤如下： $$y(x)=g(\sum_{i=1}^{n}w_ix_i+b)$$ 其中，y(x)表示神经网络的输出，g表示‌激活函数，w_i表示第i个输入数据的权重，x_i表示第i个输数据号，b为偏置项。前向传播的主要目的是基于当前的权重和偏置参数，计算模型对于给定输入的预测输出。&#xA;激活函数&#xA;在神经网络中除了线性层外往往还需要加入激活函数，没有激活函数的神经网络只是线性函数，无法拟合曲线，任何一条曲线都能用多条线性函数来逼近，而激活函数使神经网络具有分段的能力，根据万能逼近原理(universal approximation theorrm)，3层神经网络可以逼近任意曲线。以下是神经网络中最常见的三个激活函数： (1) Sigmoid、tanh、Relu激活函数： $$ \begin{align} &amp;Sigmoid(x)=\frac{1}{1+e^{-x}}\\ &amp;Tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\ &amp;Relu(x)= \begin{cases} x,x≥0\\ 0,x&lt;0 \end{cases} \end{align} $$ Relu函数反向传播求导简单，减少计算量</description>
    </item>
  </channel>
</rss>
